{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Math for ML PS 4 Programming Part\n",
        "[25 points] In this programming part, we'll solve the least squares optimization problem with gradient descent and verify that it corresponds to the one-shot analytical solution we've been seeing all semester. We will also do the same with ridge regression.\n",
        "\n",
        "To submit, just fill in all the code cells with:\n",
        "\n",
        "```\n",
        "# YOUR CODE HERE #\n",
        "```\n",
        "\n",
        "or the appropriate variations (the problems will guide you in this).\n",
        "\n",
        "You should turn in this `.ipynb` notebook into Gradescope per the homework submission instructions on the course webpage."
      ],
      "metadata": {
        "id": "wJ6yyfn9ts_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RKMp5attYpv"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving OLS Analytically\n",
        "In the following exercises, we will compare solving OLS analytically with solving it with gradient descent. First, we will solve OLS analytically using our familiar solution:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{w}} = (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{y}.\n",
        "$$"
      ],
      "metadata": {
        "id": "KgQTCIoo0n3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1 [4 points]\n",
        "For the following exercise, you will implement a function that outputs the analytical solution of OLS.\n",
        "\n",
        "Write a function below, called `analytical_ols`, with the following signature:\n",
        "- **Input:**\n",
        "  1. `X`: A `numpy` array of shape `(n, d)` containing the training data with $n$ examples and $d$ features.\n",
        "  2. `y`: A `numpy` array of shape `(n,)` containing the labels.\n",
        "\n",
        "- **Output:**\n",
        "  1. `w`: A `numpy` array of shape `(d,)` containing the learned weights.\n",
        "\n",
        "The function shouldn't be much more than a single line.\n",
        "\n",
        "Because this function will be important for the rest of the problem set, we have provided two test cases for you to try `analytical_ols` on. If all of the tests below pass with `True`, then you should be reasonably confident that your analytical OLS implementation is correct."
      ],
      "metadata": {
        "id": "Nzcin2ATeBYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analytical_ols(X, y):\n",
        "    # YOUR CODE HERE #\n",
        "\n",
        "    return w"
      ],
      "metadata": {
        "id": "B2ISCBhGe4W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST CASES:\n",
        "X, y = make_regression(n_samples=10, n_features=2, random_state=0)\n",
        "w_analytical = analytical_ols(X, y)\n",
        "true_w = np.linalg.lstsq(X, y, rcond=None)[0]\n",
        "print(\"w_analytical is correct: {}\".format(np.allclose(w_analytical, true_w)))\n",
        "X, y = make_regression(n_samples=100, n_features=5, random_state=1)\n",
        "w_analytical = analytical_ols(X, y)\n",
        "true_w = np.linalg.lstsq(X, y, rcond=None)[0]\n",
        "print(\"w_analytical is correct: {}\".format(np.allclose(w_analytical, true_w)))"
      ],
      "metadata": {
        "id": "3_2Nu85be_s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving OLS with Gradient Descent\n",
        "Now, we'll solve OLS with gradient descent. Recall the general definition of the gradient descent algorithm:\n",
        "\n",
        "**Input:** Function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$. Initial point $\\mathbf{x}_0 \\in \\mathbb{R}^d$. Step size $\\eta \\in \\mathbb{R}$. Convergence tolerance $\\epsilon > 0$.\n",
        "\n",
        "For $t = 1, 2, 3, \\dots$\n",
        "- Compute $\\mathbf{x}_t \\leftarrow \\mathbf{x}_{t - 1} - \\eta \\nabla f(\\mathbf{x}_{t - 1})$.\n",
        "- If $\\|\\mathbf{x}_t - \\mathbf{x}_{t - 1}\\| < \\epsilon$, **return** $f(\\mathbf{x}_t)$.\n",
        "\n",
        "In PS3, you coded a general version of gradient descent. In this problem, you'll code gradient descent tailored specifically for the least squares objective:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{w}) = \\| \\mathbf{X} \\mathbf{w} - \\mathbf{y} \\|^2.\n",
        "$$\n",
        "\n",
        "To do this, you'll need to hard-code the gradient. Refer to the lecture notes (or just derive yourself!) the form of the gradient."
      ],
      "metadata": {
        "id": "iqT0st24sFHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2 [6 points]\n",
        "In this exercise, you'll code gradient descent from scratch, but tailored to the least squares objective. That is, you will hard code the gradient and the objective function evaluation directly into the function `gd_ols`.\n",
        "\n",
        "Write a function below, called `gd_ols`, with the following signature:\n",
        "- **Input:**\n",
        "  1. `X`: A `numpy` array of shape `(n,d)`.\n",
        "  2. `y`: A `numpy` array of shape `(n,)`.\n",
        "  3. `w0`: A `numpy` array of shape `(d,)` for the initial point, $\\mathbf{w}_0 \\in \\mathbb{R}^d$.\n",
        "  3. `eta`: A `float` for the step size $\\eta \\in \\mathbb{R}$.\n",
        "  4. `eps`: A `float` representing $\\epsilon \\in \\mathbb{R}$ for the stopping condition, $\\| \\mathbf{x}_t - \\mathbf{x}_{t-1}\\| < \\epsilon$.\n",
        "  5. `max_iter`: The maximum number of iterations to perform gradient descent until forcing the algorithm to return. We will set the default to `500`.\n",
        "\n",
        "- **Output:**\n",
        "  1. `w_T`: The $\\mathbf{w}_T \\in \\mathbb{R}^d$ that is returned by gradient descent after either reaching the stopping condition or `max_iter` iterations. This should be a `numpy` array of shape `(d,)`.\n",
        "  2. `f_T`: The function value at `w_T`. This should be a `float`.\n",
        "  3. `ws`: A `list` of `numpy` arrays of shape `(d,)` that holds all the input values $\\mathbf{w}_0, \\mathbf{w}_1, \\mathbf{w}_2, \\dots$ of gradient descent, starting with `w0`.\n",
        "  4. `fs`: A `list` of `numpy` arrays of shape `(d,)` that holds all the function values of gradient descent $f(\\mathbf{w}_0), f(\\mathbf{w}_1), \\dots$, starting with the least squares objective evaluated at `x0`.\n",
        "\n",
        "The function below is set up to return `w_T`, `f_T`, `ws`, and `fs` already (Python functions can return multiple return values, techincally as a `tuple`)."
      ],
      "metadata": {
        "id": "S8yYfHynoflq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gd_ols(X, y, w0, eta, eps, max_iter=500):\n",
        "    # YOUR CODE HERE #\n",
        "\n",
        "    return w_T, f_T, ws, fs"
      ],
      "metadata": {
        "id": "by_gpiavleJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3 [6 points]\n",
        "Below, we construct a regression problem with $n = 1000$ and $d = 50$.\n",
        "\n",
        "In this exercise, first use `analytical_ols` to get an exact solution $\\mathbf{w}^* \\in \\mathbb{R}^d$ to the regression problem. Then, run `gd_ols` on the same regression problem, with the hyperparemeters:\n",
        "\n",
        "- `w0`: The all zeros vector, $\\mathbf{w}_0 = \\mathbf{0}$.\n",
        "- `eta`: The principled choice of learning rate suggested by the theorem given in class, $\\eta = 1/\\beta$, where $\\beta$ is the maximum eigenvalue of the Hessian, $2 \\mathbf{X}^{\\top} \\mathbf{X}$.\n",
        "- `eps`: `1e-4`.\n",
        "- `max_iter`: `500`, the default.\n",
        "\n",
        "Finally, for all $T$ iterations of your `gd_ols` run, compute the distance between $\\mathbf{w}_t$ and $\\mathbf{w}^*$, given by\n",
        "\n",
        "$$\n",
        "\\| \\mathbf{w}_t - \\mathbf{w}^* \\|,\n",
        "$$\n",
        "\n",
        "and plot these using `matplotlib` with the timestep $t = 1, 2, \\dots, T$ on the $x$-axis and $\\| \\mathbf{w}_t - \\mathbf{w}^* \\|$ on the $y$-axis."
      ],
      "metadata": {
        "id": "_WqCDbw2qD54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_regression(n_samples=1000, n_features=50, random_state=0)"
      ],
      "metadata": {
        "id": "dnXrZa1jqDZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "kc9w9KHSleG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4 [4 points]\n",
        "In lecture, our main theorem for gradient descent on convex functions applied to OLS gave us this guarantee on the convergence of the function values after $T$ iterations:\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{X}\\mathbf{w}_T - \\mathbf{y}\\|^2 - \\|\\mathbf{X}\\mathbf{w}^* - \\mathbf{y}\\|^2 \\leq \\frac{\\beta}{2T} \\left( \\|\\mathbf{w}_0 - \\mathbf{w}^*\\|^2 - \\|\\mathbf{w}_T - \\mathbf{w}^*\\|^2 \\right).\n",
        "$$\n",
        "\n",
        "In this exercise, we will investigate just how tight the \"gap\" between the left and right hand sides are. Using the same hyperparameters as Exercise 3 for `gd_ols`, vary $T = 1, \\dots, 100$ using `max_iter` and compute the left-hand side\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{X}\\mathbf{w}_T - \\mathbf{y}\\|^2 - \\|\\mathbf{X}\\mathbf{w}^* - \\mathbf{y}\\|^2\n",
        "$$\n",
        "\n",
        "and right-hand side\n",
        "\n",
        "$$\n",
        "\\frac{\\beta}{2T} \\left( \\|\\mathbf{w}_0 - \\mathbf{w}^*\\|^2 - \\|\\mathbf{w}_T - \\mathbf{w}^*\\|^2 \\right).\n",
        "$$\n",
        "\n",
        "of the above bound. Store these `100` computations into the lists `lhs` and `rhs`, respectively. Don't forget to keep the squares in these quantities!\n",
        "\n",
        "Your $\\mathbf{w}^*$ should be obtained from `analytical_ols`, and $\\mathbf{w}_T$ should be obtained from the last iteration of `gd_ols`, in the variable `w_T`. The quantity $\\|\\mathbf{X}\\mathbf{w}_T - \\mathbf{y}\\|^2$ can just be obtained from `f_T` if your implementation of `gd_ols` is correct.\n",
        "\n",
        "Then, plot the computations in `lhs` and `rhs` on the same plot using `matplotlib`. The $x$-axis should be the integers $1$ to $100$ for the values of $T$. The $y$-axis should be the `float` values from `lhs` and `rhs`. Make sure this plot is included in your submission. It may be helpful to [consult the matplotlib documentation](https://matplotlib.org/stable/gallery/pyplots/pyplot_three.html) if you need help plotting both lines.\n"
      ],
      "metadata": {
        "id": "3RzCGetUsj7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_regression(n_samples=1000, n_features=50, random_state=0)"
      ],
      "metadata": {
        "id": "IxYvfCLR_L2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "X1hdkdYwuFZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 5 [5 points]\n",
        "In this exercise, you will modify your implementation of `gd_ols` to handle Ridge Regression. You may copy and paste your implementation from Exercise 2 here and make the appropriate modification (the gradient step should change).\n",
        "\n",
        "Recall from class that the Ridge Regression objective is the function:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{w}) = \\| \\mathbf{X} \\mathbf{w} - \\mathbf{y} \\|^2 + \\gamma \\| \\mathbf{w} \\|^2\n",
        "$$\n",
        "\n",
        "for some $\\gamma > 0$. Ridge regression is convex, so we can be ensured that gradient descent converges with this objective. In this exercise, write a function `gd_ridge` that implements Ridge Regression, again hardcoding the gradient and function evaluation into `gd_ridge`. It should take in all the same arguments, with an additional argument `gamma` for the regularization. In all, the function should have the following signature:\n",
        "\n",
        "- **Input:**\n",
        "  1. `X`: A `numpy` array of shape `(n,d)`.\n",
        "  2. `y`: A `numpy` array of shape `(n,)`.\n",
        "  3. `w0`: A `numpy` array of shape `(d,)` for the initial point, $\\mathbf{w}_0 \\in \\mathbb{R}^d$.\n",
        "  3. `eta`: A `float` for the step size $\\eta \\in \\mathbb{R}$.\n",
        "  4. `eps`: A `float` representing $\\epsilon \\in \\mathbb{R}$ for the stopping condition, $\\| \\mathbf{x}_t - \\mathbf{x}_{t-1}\\| < \\epsilon$.\n",
        "  5. `gamma`: A `float` representing the $\\gamma > 0$ regularization strength in ridge, the coefficient in front of $\\| \\mathbf{w} \\|^2$.\n",
        "  6. `max_iter`: The maximum number of iterations to perform gradient descent until forcing the algorithm to return. We will set the default to `500`.\n",
        "\n",
        "- **Output:**\n",
        "  1. `w_T`: The $\\mathbf{w}_T \\in \\mathbb{R}^d$ that is returned by gradient descent after either reaching the stopping condition or `max_iter` iterations. This should be a `numpy` array of shape `(d,)`.\n",
        "  2. `f_T`: The function value at `w_T`. This should be a `float`.\n",
        "  3. `ws`: A `list` of `numpy` arrays of shape `(d,)` that holds all the input values $\\mathbf{w}_0, \\mathbf{w}_1, \\mathbf{w}_2, \\dots$ of gradient descent, starting with `w0`.\n",
        "  4. `fs`: A `list` of `numpy` arrays of shape `(d,)` that holds all the function values of gradient descent $f(\\mathbf{w}_0), f(\\mathbf{w}_1), \\dots$, starting with the least squares objective evaluated at `x0`.\n",
        "\n",
        "The function below is set up to return `w_T`, `f_T`, `ws`, and `fs` already (Python functions can return multiple return values, techincally as a `tuple`).\n",
        "\n",
        "Then, run `gd_ridge` on the supplied regression problem with $n = 1000$ and $d = 50$ to obtain $\\mathbf{w}_{ridge}$. Compute\n",
        "\n",
        "$$\n",
        "\\| \\mathbf{X} \\mathbf{w}_{ridge} - \\mathbf{y} \\|^2,\n",
        "$$\n",
        "\n",
        "and compare it to\n",
        "\n",
        "$$\n",
        "\\| \\mathbf{X} \\mathbf{w}^* - \\mathbf{y} \\|^2.\n",
        "$$\n",
        "\n",
        "Print both values."
      ],
      "metadata": {
        "id": "vRwiwGdm0eOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_regression(n_samples=1000, n_features=50, random_state=0)"
      ],
      "metadata": {
        "id": "lYZ_-IQu0PA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #\n",
        "def gd_ridge(X, y, w0, eta, eps, gamma=1.0, max_iter=500):\n",
        "    # YOUR CODE HERE #\n",
        "\n",
        "    return w_T, f_T, ws, fs"
      ],
      "metadata": {
        "id": "A9Keond5-vmv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}