{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Math for ML PS 1 Programming Part\n",
        "[18 points] In this programming part, you will apply the techniques you learned in class for least squares regression to a couple of simple examples. A couple of these examples are modifications of exercises from Jake VanderPlas' excellent [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html).\n",
        "\n",
        "To submit, just fill in all the code cells with:\n",
        "\n",
        "```\n",
        "# YOUR CODE HERE #\n",
        "```\n",
        "\n",
        "You should turn in this `.ipynb` notebook into Gradescope per the homework sbumission instructions on the course webpage."
      ],
      "metadata": {
        "id": "wJ6yyfn9ts_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RKMp5attYpv"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression with $d = 1$\n",
        "When $d = 1$, our data matrix is just a column vector, $\\mathbf{x} \\in \\mathbb{R}^{n \\times 1}$. We want to find $w \\in \\mathbb{R}$ that minimizes the sum of squared residuals error:\n",
        "\n",
        "$$\n",
        "\\| w\\mathbf{x} - y\\|^2.\n",
        "$$\n",
        "\n",
        "In this case, we are finding a line of best fit to our data.\n",
        "\n",
        "Consider the following $n =50$ randomly generated datapoints."
      ],
      "metadata": {
        "id": "6ZnuPMkTuapd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.RandomState(1)\n",
        "x = 10 * rng.rand(50)\n",
        "y = 2 * x + rng.randn(50)\n",
        "plt.scatter(x, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iHJGtsTauYge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1 [4 points]\n",
        "Write code below to find our weight $\\hat{w} \\in \\mathbb{R}$ using the ordinary least squares solution from lecture. Then, get the predictions $\\hat{\\mathbf{y}} \\in \\mathbb{R}^{50}$ using $\\hat{w}$. Finally, compute the sum of squared residuals error with $\\mathbf{y} \\in \\mathbb{R}^{50}$, the true labels. Print all three of these quantities.\n",
        "\n",
        "Appending `.T` to a `numpy` array transposes the array."
      ],
      "metadata": {
        "id": "364QOKPFv3Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "Zz5oIgrJwY_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we want to also fit data that has an intercept, we do the simple preprocessing step described in class. Consider the following $n = 50$ datapoints, now shifted up a bit."
      ],
      "metadata": {
        "id": "KhlGrfVlxVfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 10 * rng.rand(50)\n",
        "y = 2 * x + 3 + rng.randn(50)\n",
        "plt.scatter(x, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9W9JcIS-yAko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2 [4 points]\n",
        "Preprocess the data to fit the intercept, as described in lecture. To do this, we add a \"dummy\" dimension of all $1$s, so we have the data matrix\n",
        "\n",
        "$$\n",
        "\\mathbf{X} = \\begin{bmatrix}\n",
        "x_1 & 1 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "x_n & 1\n",
        "\\end{bmatrix}\n",
        "\\in \\mathbb{R}^{50 \\times 2}\n",
        "$$\n",
        "\n",
        "and we fit a weight vector $\\mathbf{w} \\in \\mathbb{R}^2$, where we take the second entry to be the value for the intercept, $w_0 \\in \\mathbb{R}$:\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = \\begin{bmatrix}\n",
        "w \\\\\n",
        "w_0\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "For this exercise, preprocess the data in `x` as described above. Ensure that the preprocessed matrix $\\mathbf{X} \\in \\mathbb{R}^{50 \\times 2}$ with `.shape`. Then, again fit $\\mathbf{w} \\in \\mathbb{R}^2$ using the OLS solution, find the predictions $\\hat{\\mathbf{y}}$, and compute the sum of squared residuals error. Print $\\mathbf{w}$, the predictions $\\hat{\\mathbf{y}}$, and the error.\n",
        "\n",
        "The function `np.linalg.inv()` should be helpful for taking matrix inverses."
      ],
      "metadata": {
        "id": "_ndWyJx0yNqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "8N_DEKReyJ2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the line of best fit we found using the function `plot_best_fit` below. This takes `x`, `y` (from the original data above), and `w` (which you found in Exercise 2). To ensure that `plot_best_fit` works, `w` should be an `np.array` containing 2 numbers."
      ],
      "metadata": {
        "id": "rO__7KPb4eqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_best_fit(x, y, w, n=1000):\n",
        "  xfit = np.linspace(0, 10, n)\n",
        "  xfit_t = np.hstack((xfit.reshape(n, 1), np.ones((n, 1))))\n",
        "  yfit = xfit_t @ w.reshape(2, 1)\n",
        "  plt.scatter(x, y)\n",
        "  plt.plot(xfit, yfit.reshape(n,1))"
      ],
      "metadata": {
        "id": "CLCYDeg00fx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL:\n",
        "# Using the w you found in Exercise 2, use plot_best_fit\n",
        "# to visualize your line of best fit to x and y."
      ],
      "metadata": {
        "id": "SqrsHSz73nC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression with $d = 2$\n",
        "Of course, least squares regression generalizes to higher dimensions. Here's an example of $n = 100$ datapoints with $d = 2$.\n"
      ],
      "metadata": {
        "id": "x60YKDwEzxE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = 2 * rng.rand(100, 2)\n",
        "y = 0.5 + np.dot(X, [8, -1.]) + np.random.normal(size=(100,))\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(X[:,0], X[:,1], y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qOO73qgtzvuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3 [4 points]\n",
        "Preprocess the data to fit the intercept, as described in lecture. To do this, we add a \"dummy\" dimension of all $1$, so we have the data matrix\n",
        "\n",
        "$$\n",
        "\\mathbf{X} = \\begin{bmatrix}\n",
        "\\uparrow & \\uparrow & 1 \\\\\n",
        "\\mathbf{x}_1 & \\mathbf{x}_2 & \\vdots \\\\\n",
        "\\downarrow & \\downarrow & 1\n",
        "\\end{bmatrix} \\in \\mathbb{R}^{100 \\times 3}\n",
        "$$\n",
        "\n",
        "and we fit a weight vector $\\mathbf{w} \\in \\mathbb{R}^3$, where we take the third entry to be the value for the intercept, $w_0 \\in \\mathbb{R}$:\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = \\begin{bmatrix}\n",
        "w_1 \\\\\n",
        "w_2 \\\\\n",
        "w_0\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "For this exercise, preprocess the data in `X` as described above. Then, again fit $\\mathbf{w} \\in \\mathbb{R}^3$ using the OLS solution, find the predictions $\\hat{\\mathbf{y}}$, and compute the sum of squared residuals error. Print $\\mathbf{w}$, the predictions $\\hat{\\mathbf{y}}$, and the error.\n",
        "\n",
        "The function `np.linalg.inv()` should be helpful for taking matrix inverses."
      ],
      "metadata": {
        "id": "xXFclV-47JS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "VZtv7vdh64vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the plane of best fit we found using the function `plot_plane` below. This takes `X`, `y` (the original data above), and `w` (which you found in Exercise 3). To ensure that `plot_plane` works, `w` should be an `np.array` containing 3 numbers."
      ],
      "metadata": {
        "id": "qH-FhBCO9goJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_plane(X, y, w, n=1000):\n",
        "  xsurf, ysurf = np.meshgrid(np.linspace(0, 2), np.linspace(0, 2))\n",
        "  zsurf = w[0] * xsurf + w[1] * ysurf + w[2]\n",
        "\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(projection='3d')\n",
        "  ax.scatter(X[:,0], X[:,1], y)\n",
        "  ax.plot_surface(xsurf, ysurf, zsurf,\n",
        "                  rstride=1, cstride=1, color='b', alpha=0.3)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "rV9qm2BY8Wi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression with $d = 10$\n",
        "Now we'll finish with a real-world example of data: the diabetes dataset from `sklearn`, a popular machine learning library. This dataset has $d = 10$ measurements for $n = 442$ diabetes patients. The label, $y$, is a quantitative measure of disease progression one year after baseline.\n",
        "\n",
        "The $d = 10$ features are:\n",
        "\n",
        "* `age`: age in years\n",
        "* `sex`: sex (m/f)\n",
        "* `bmi`: body mass index\n",
        "* `bp`: average blood pressure\n",
        "* `s1 tc`: total serum cholestrol\n",
        "* `s2 ldl`: low-density liboproteins\n",
        "* `s3 hdl`: high-density liboproteins\n",
        "* `s4 tch`: total cholestrol / HDL\n",
        "* `s5 ltg`: possibly log of serum triglycerides level\n",
        "* `s6 glu` : blood sugar level\n",
        "\n",
        "**NOTE:** All of these feature values have been normalized through the $n = 442$ exampples by subtracting the mean of each column and dividing by the column's standard deviation. This explains why the scale for each of the features is not what you'd expect.\n"
      ],
      "metadata": {
        "id": "kIBIHHOi9xSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "import pandas as pd\n",
        "\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "diabetes = load_diabetes()\n",
        "diabetes_df = pd.DataFrame(data=diabetes.data,\n",
        "                           columns=diabetes.feature_names)\n",
        "diabetes_df['y'] = diabetes.target"
      ],
      "metadata": {
        "id": "yQQUjUVD-Pkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the first $10$ data samples from the $n = 442$ total."
      ],
      "metadata": {
        "id": "7A1UK8YoCnPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df.head(10)"
      ],
      "metadata": {
        "id": "BUiLhRmA-W7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the $d = 10$ features, we can plot histograms of a select few of them. These histograms visualize each feature across all $n = 442$ examples. Let's take a look at the features:\n",
        "* `age`: age in years\n",
        "* `sex`: sex (m/f)\n",
        "* `bmi`: body mass index\n",
        "* `bp`: average blood pressure"
      ],
      "metadata": {
        "id": "FQoyvzo8DBUn"
      }
    },
    {
      "source": [
        "diabetes_df['age'].plot(kind='hist', bins=20, title='age')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "PzCrXPkmC1iO"
      }
    },
    {
      "source": [
        "diabetes_df['sex'].plot(kind='hist', bins=20, title='sex')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "5oWlLEP-DegU"
      }
    },
    {
      "source": [
        "diabetes_df['bmi'].plot(kind='hist', bins=20, title='bmi')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "jN4ZpeL9C6Ff"
      }
    },
    {
      "source": [
        "diabetes_df['bp'].plot(kind='hist', bins=20, title='bp')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "K7lxO7tzC84x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The matrix `X` and the vector `y` contain the data already in the forms we'd like for regression."
      ],
      "metadata": {
        "id": "B1em0rawDsRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)"
      ],
      "metadata": {
        "id": "_AR8UjdjBhAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.shape)"
      ],
      "metadata": {
        "id": "Wf_SjSQfEBFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4 [4 points]\n",
        "Preprocess the data to fit the intercept, as described in lecture. To do this, we add a \"dummy\" dimension of all $1$, so we have the data matrix\n",
        "\n",
        "$$\n",
        "\\mathbf{X} = \\begin{bmatrix}\n",
        "\\uparrow & \\uparrow & \\uparrow & 1 \\\\\n",
        "\\mathbf{x}_1 & \\dots & \\mathbf{x}_d & \\vdots \\\\\n",
        "\\downarrow & \\downarrow & \\downarrow & 1\n",
        "\\end{bmatrix} \\in \\mathbb{R}^{442 \\times 11}\n",
        "$$\n",
        "\n",
        "and we fit a weight vector $\\mathbf{w} \\in \\mathbb{R}^{11}$, where we take the eleventh entry to be the value for the intercept, $w_0 \\in \\mathbb{R}$:\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = \\begin{bmatrix}\n",
        "w_1 \\\\\n",
        "\\vdots \\\\\n",
        "w_{10} \\\\\n",
        "w_0\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "For this exercise, preprocess the data in `X` as described above. Then, again fit $\\mathbf{w} \\in \\mathbb{R}^{11}$ using the OLS solution, find the predictions $\\hat{\\mathbf{y}}$, and compute the sum of squared residuals error. Print $\\mathbf{w}$, the predictions $\\hat{\\mathbf{y}}$, and the error.\n",
        "\n",
        "The function `np.linalg.inv()` should be helpful for taking matrix inverses."
      ],
      "metadata": {
        "id": "xBlX64z5EOW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WRITE CODE HERE #"
      ],
      "metadata": {
        "id": "uaY0_2GAEBo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 5 [2 points]\n",
        "The primary machine learning library is scikit-learn, imported with `sklearn`. This library contains a plethora of ML algorithms ready for you to use. This exercise will introduce you to the documentation in `sklearn` and the basic pipeline for building a machine learning model with this library. The [scikit-learn documentation](https://scikit-learn.org/stable/index.html) is rich and contains more or less everything you need to do basic machine learning tasks.\n",
        "\n",
        "In this exercise, we'll use `sklearn`'s already-implemented regression functionality and compare it to our solution as a sanity check. The documentation for `LinearRegression` can [be found here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
        "\n",
        "Read through the documentation for `LinearRegression` (the \"Examples\" section will help). In the `# YOUR CODE HERE #` cell, write code that fits a linear regression model using `.fit()` and print the weights using `.coef_` and `.intercept_`. Compare these to the weights you found in Exercise 4 (they should match exactly).\n",
        "\n",
        "*Optional:* If you're curious, check out what `LinearRegression` is doing [under the hood here](https://github.com/scikit-learn/scikit-learn/blob/2621573e6/sklearn/linear_model/_base.py#L465). Can you find where the familiar OLS solution in lecture is located?"
      ],
      "metadata": {
        "id": "w_AuWOWlFbYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "OSu1rgntH2QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "nrdnhCepE3YL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}