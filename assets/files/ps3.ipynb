{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Math for ML PS 3 Programming Part\n",
        "[25 points] In this programming part, you will implement gradient descent from scratch and analyze its behavior on a few simple functions. Then, you'll use it on a real dataset and analyze its behavior on that dataset.\n",
        "\n",
        "To submit, just fill in all the code cells with:\n",
        "\n",
        "```\n",
        "# YOUR CODE HERE #\n",
        "```\n",
        "\n",
        "or the appropriate variations (the problems will guide you in this).\n",
        "\n",
        "You should turn in this `.ipynb` notebook into Gradescope per the homework submission instructions on the course webpage."
      ],
      "metadata": {
        "id": "wJ6yyfn9ts_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RKMp5attYpv"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Gradient Descent\n",
        "In this first part, you will write a function that implements gradient descent from scratch. Recall from lecture that the pseudocode for gradient descent is as follows:\n",
        "\n",
        "**Input:** Function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$. Initial point $\\mathbf{x}_0 \\in \\mathbb{R}^d$. Step size $\\eta \\in \\mathbb{R}$.\n",
        "\n",
        "For $t = 1, 2, 3, \\dots$\n",
        "- Compute $\\mathbf{x}_t \\leftarrow \\mathbf{x}_{t - 1} - \\eta \\nabla f(\\mathbf{x}_{t - 1})$.\n",
        "- If $\\mathbf{x}_t - \\mathbf{x}_{t - 1}$ is sufficiently small, **return** $\\nabla f(\\mathbf{x}_t)$.\n",
        "\n",
        "In this problem, we'll deal with functions that take as input a point in $\\mathbf{x} \\in \\mathbb{R}^d$ encoded as a `numpy` array and returns a scalar value in $\\mathbb{R}$, which will be a `float`.\n",
        "\n",
        "For example, the following code cell defines the function $f(x_1, x_2) = x_1^2 + x_2^2$."
      ],
      "metadata": {
        "id": "6ZnuPMkTuapd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    return x[0] ** 2 + x[1] ** 2"
      ],
      "metadata": {
        "id": "Mp8N11qm0luz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider a couple of values\n",
        "x1 = np.array([1, 2])\n",
        "x2 = np.array([3, 4])\n",
        "print(\"The value of f at (1,2) is {}\".format(f(x1)))\n",
        "print(\"The value of f at (3,4) is {}\".format(f(x2)))"
      ],
      "metadata": {
        "id": "cPWkUJIX3cGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.linspace(-1, 1, 100)\n",
        "Y = np.linspace(-1, 1, 100)\n",
        "X, Y = np.meshgrid(X, Y)\n",
        "Z = f([X, Y])\n",
        "\n",
        "# Plot in 3D\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X, Y, Z, facecolors = cm.jet(Z/np.amax(Z)))\n",
        "ax.set_xlabel('$x_1$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vtfylZhh2vIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cmap = plt.colormaps[\"jet\"]\n",
        "plt.contourf(X, Y, Z, 100, cmap=cmap)\n",
        "plt.colorbar()\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gl2RJ3rd3BUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the convention for functions in this problem uniform, we'll make sure that even single-variable functions take as input a `numpy` array of length `1`, outputting a scalar `float`.\n",
        "\n",
        "For example, take the following function $f(x) = x^2$."
      ],
      "metadata": {
        "id": "47nXW9O73wEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    return x[0] ** 2"
      ],
      "metadata": {
        "id": "61MSxdJw3u3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider a couple of values\n",
        "x1 = np.array([1])\n",
        "x2 = np.array([3])\n",
        "print(\"The value of f at 1 is {}\".format(f(x1)))\n",
        "print(\"The value of f at 3 is {}\".format(f(x2)))"
      ],
      "metadata": {
        "id": "21x5BWJz3-0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.linspace(-1, 1, 100).reshape(100, 1)\n",
        "Y = f([X])\n",
        "plt.plot(X, Y)\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$f(x)$')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KPMTtN7a4DBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also deal with these functions' gradients, encoded as Python functions that take as input a `numpy` array in $\\mathbb{R}^d$, the input space of the function. They will output a `numpy` array in $\\mathbb{R}^d$ of shape `(d,)`, the gradient as a `numpy` array.\n",
        "\n",
        "For example, the gradient of $f(x_1, x_2) = x_1^2 + x_2^2$ above is\n",
        "\n",
        "$$\n",
        "\\nabla f(\\mathbf{x}) = \\begin{bmatrix}\n",
        "2 x_1 \\\\\n",
        "2 x_2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The following function implements this."
      ],
      "metadata": {
        "id": "KPYbeczY6jC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_f(x):\n",
        "    return np.array([2 * x[0], 2 * x[1]])"
      ],
      "metadata": {
        "id": "R7LnUcrQ7EK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The gradient of f at (1,2) is: {}\".format(grad_f(np.array([1, 2]))))\n",
        "print(\"The gradient of f at (3,4) is: {}\".format(grad_f(np.array([3, 4]))))"
      ],
      "metadata": {
        "id": "1vql_mcs7F30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, for single-variable functions, we'll follow the same convention. That is, single variable gradient functions will take a `numpy` with a single entry. They will output a `numpy` array with a single entry."
      ],
      "metadata": {
        "id": "_5DDxjWz6_zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_f(x):\n",
        "    return np.array([2 * x[0]])"
      ],
      "metadata": {
        "id": "2A90c2FC71oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The gradient of f at 1 is: {}\".format(grad_f(np.array([1]))))\n",
        "print(\"The gradient of f at 3 is: {}\".format(grad_f(np.array([3]))))"
      ],
      "metadata": {
        "id": "bAmkqiKb74DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our implementation of gradient descent will take in five arguments:\n",
        "\n",
        "\n",
        "1. `f`: A function that takes as input a `numpy` array and outputs a `float`, as above.\n",
        "2. `grad_f`: A function that takes as input a `numpy` array and outputs a `numpy` array, as above.\n",
        "3. `eta`: A `float` for the step size $\\eta \\in \\mathbb{R}$.\n",
        "4. `x0`: A `numpy` array for the initial point, $\\mathbf{x}_0 \\in \\mathbb{R}^d$.\n",
        "5. `eps`: A `float` representing $\\epsilon \\in \\mathbb{R}$ for the stopping condition, $\\| \\mathbf{x}_t - \\mathbf{x}_{t - 1}\\| \\leq \\epsilon$.\n",
        "\n",
        "The following couple of exercises will guide you through building this function up bit by bit.\n",
        "\n"
      ],
      "metadata": {
        "id": "ivC8uL6x4AMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1 [2 points]\n",
        "Write a function below, called `check_stop` with the following signature:\n",
        "\n",
        "\n",
        "*   **Input:** Two `numpy` arrays, `a` and `b`, both of shape `(d,)`, and a `float` denoted `eps`.\n",
        "*   **Output:** A `bool` value, either `True` or `False`.\n",
        "\n",
        "Your function should check if the Euclidean norm between $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^d$ is below the threshold $\\epsilon$. It should output `True` if it is below and `False` otherwise.\n",
        "\n"
      ],
      "metadata": {
        "id": "L91nQJXj8E_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_stop(a, b, eps):\n",
        "    # YOUR CODE HERE #\n",
        "    return False"
      ],
      "metadata": {
        "id": "isCoMtVJ8K3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It should return `True` on the first test case below and `False` on the second test case below."
      ],
      "metadata": {
        "id": "FmHDrg3B9th9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"check_stop should print True: {}\".format(check_stop(np.array([1, 2]), np.array([1, 2]), 0.1)))\n",
        "print(\"check_stop should print False: {}\".format(check_stop(np.array([1, 2]), np.array([1, 3]), 0.1)))"
      ],
      "metadata": {
        "id": "uyBjBeF-9zar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2 [6 points]\n",
        "Write a function below, called `grad_desc`, with the following signature:\n",
        "- **Input:**\n",
        "  1. `f`: A function that takes as input a `numpy` array of shape `(d,)` and outputs a `float`, as above.\n",
        "  2. `grad_f`: A function that takes as input a `numpy` array and outputs a `numpy` array of shape `(d,)`, as above.\n",
        "  3. `eta`: A `float` for the step size $\\eta \\in \\mathbb{R}$.\n",
        "  4. `x0`: A `numpy` array for the initial point, $\\mathbf{x}_0 \\in \\mathbb{R}^d$. This array should be shape `(d,)`.\n",
        "  5. `eps`: A `float` representing $\\epsilon \\in \\mathbb{R}$ for the stopping condition, $\\| \\mathbf{x}_t - \\mathbf{x}_{t - 1}\\| \\leq \\epsilon$.\n",
        "  6. `max_iter`: The maximum number of iterations to perform gradient descent until forcing the algorithm to return. We will set the default to `500`.\n",
        "\n",
        "- **Output:**\n",
        "  1. `x_T`: The $\\mathbf{x} \\in \\mathbb{R}^d$ that is returned by gradient descent after either reaching the stopping condition or `max_iter` iterations. This should be a `numpy` array of shape `(d,)`.\n",
        "  2. `f_T`: The function value at `x_T`. This should be a `float`.\n",
        "  3. `xs`: A `list` of `numpy` arrays of shape `(d,)` that holds all the input values $\\mathbf{x}_0, \\mathbf{x}_1, \\mathbf{x}_2, \\dots$ of gradient descent, starting with `x0`.\n",
        "  4. `fs`: A `list` of `numpy` arrays of shape `(d,)` that holds all the function values of gradient descent $f(\\mathbf{x}_0), f(\\mathbf{x}_1), \\dots$, starting with `f` evaluated at `x0`.\n",
        "\n",
        "The function below is set up to return `x_T`, `f_T`, `xs`, and `fs` already (Python functions can return multiple return values, techincally as a `tuple`). You may use the `check_stop` function above in the implementation of `grad_desc`.\n",
        "\n",
        "Because this function will be important for the rest of the problem set, we have provided two test cases for you to try `grad_desc` on. If all of the tests below pass with `True`, then you should be reasonably confident that your gradient descent implementation is correct."
      ],
      "metadata": {
        "id": "wjAGhLwS-_we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_desc(f, grad_f, eta, x0, eps, max_iter=500):\n",
        "    xs = []\n",
        "    fs = []\n",
        "    # YOUR CODE HERE #\n",
        "    return x_T, f_T, xs, fs"
      ],
      "metadata": {
        "id": "IrP166bcCcWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function is $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$, defined as\n",
        "$$\n",
        "f(x_1, x_2) = x_1^2 + x_2^2.\n",
        "$$\n",
        "To test on this function, use the supplied paramters below."
      ],
      "metadata": {
        "id": "K-l7zGKLC8o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    return x[0] ** 2 + x[1] ** 2\n",
        "\n",
        "def grad_f(x):\n",
        "    return np.array([2 * x[0], 2 * x[1]])\n",
        "\n",
        "eta = 0.1\n",
        "x0 = np.array([1, 2])\n",
        "eps = 1e-10\n",
        "\n",
        "# TEST CASES:\n",
        "x_5, f_5, xs, fs = grad_desc(f, grad_f, eta, x0, eps, 5)\n",
        "print(\"x_5 is correct: {}\".format(np.allclose(x_5, np.array([0.32768, 0.65536]))))\n",
        "print(\"f_5 is correct: {}\".format(np.allclose(f_5, 0.53687)))\n",
        "\n",
        "x_10, f_10, xs, fs = grad_desc(f, grad_f, eta, x0, eps, 10)\n",
        "print(\"x_10 is correct: {}\".format(np.allclose(x_10, np.array([0.10737418, 0.21474836]))))\n",
        "print(\"f_10 is correct: {}\".format(np.allclose(f_10, 0.05764607523)))\n",
        "\n",
        "x_100, f_100, xs, fs = grad_desc(f, grad_f, eta, x0, eps, 100)\n",
        "print(\"x_100 is correct: {}\".format(np.allclose(x_100, np.array([0, 0]))))\n",
        "print(\"f_100 is correct: {}\".format(np.allclose(f_100, 0)))"
      ],
      "metadata": {
        "id": "iGBbcvsRC8Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function is $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, defined as\n",
        "$$\n",
        "f(x) = 2x^2 - x + 1.\n",
        "$$\n",
        "To test on this function, use the supplied parameters below."
      ],
      "metadata": {
        "id": "aAkNwrN9Fbij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    return 2 * x[0] ** 2 - x[0] + 1\n",
        "\n",
        "def grad_f(x):\n",
        "    return np.array([4 * x[0] - 1])\n",
        "\n",
        "eta = 0.1\n",
        "x0 = np.array([3])\n",
        "eps = 1e-10\n",
        "\n",
        "# TEST CASES:\n",
        "x_5, f_5, xs, fs = grad_desc(f, grad_f, eta, x0, eps, 5)\n",
        "print(\"x_5 is correct: {}\".format(np.allclose(x_5, np.array([0.46384]))))\n",
        "print(\"f_5 is correct: {}\".format(np.allclose(f_5, 0.96645509)))\n",
        "x_10, f_10, xs, fs = grad_desc(f, grad_f, eta, x0, eps, 10)\n",
        "print(\"x_10 is correct: {}\".format(np.allclose(x_10, np.array([0.2666282]))))\n",
        "print(\"f_10 is correct: {}\".format(np.allclose(f_10, 0.87555299)))\n",
        "x_100, f_100, xs, fs = grad_desc(f, grad_f, eta, x0, eps, 100)\n",
        "print(\"x_100 is correct: {}\".format(np.allclose(x_100, np.array([0.25]))))\n",
        "print(\"f_100 is correct: {}\".format(np.allclose(f_100, 0.875)))"
      ],
      "metadata": {
        "id": "XljbS8s0Fa_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent on $f: \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
        "Now that we've implemented gradient descent from scratch in the function `grad_desc`, let's try it out on several functions to verify whether it works and what the behavior of gradient descent looks like with several choices of parameters.\n",
        "\n",
        "In machine learning, *hyperparameters* are choices of parameters for algorithms that affect the behavior of the algorithm. In many cases, there are numerous hyperparameters that the user of an algorithm may \"tune\" to affect the algorithm's pefrformance. Think of these as knobs and dials that one can specify to one's liking. Typically, these hyperparameters are chosen by trial and error. In certain cases, theory guides us to specific hyperparameter choices (or, at least, principled directions to tune these knobs).\n",
        "\n",
        "In class, we've established that the main two hyperparameters of our basic implementation of gradient descent are:\n",
        "\n",
        "1. The **step size.** This is how far we take a stride at each step in gradient descent. $\\eta > 0$ usually denotes a step size, and our input parameter `eta` in `grad_desc` controls this.\n",
        "2. The **point of initialization, $\\mathbf{x}_0$**. This is where we begin our descent. The input parameter `x0` in `grad_desc` controls this.\n",
        "\n",
        "Recall from lecture that a larger step size results in a possibly faster convergence, but it may lead to instability. A different initialization might lead us to a different local minimum if a function has several local minima. In these ways, gradient descent is sensitive to these parameters.\n",
        "\n",
        "Thankfully, we proved a theorem in class that at least gives us one principled way of choosing the $\\eta$ hyperparameter, at least when our function is $\\mathcal{C}^2$ and $\\beta$-smooth. Restating that theorem here:\n",
        "\n",
        "**Theorem (Gradient descent makes the function value smaller).** Let $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ be a $\\mathcal{C}^2$, $\\beta$-smooth function. Then, for any $t = 1, 2, 3, \\dots$, gradient descent with $\\eta = 1/\\beta$ has the property:\n",
        "$$\n",
        "f(\\mathbf{x}_t) \\leq f(\\mathbf{x}_{t - 1}) - \\frac{1}{2\\beta} \\| \\nabla f(\\mathbf{x}_{t - 1})\\|^2.\n",
        "$$"
      ],
      "metadata": {
        "id": "KgQTCIoo0n3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we consider the following single-variable function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, defined as:\n",
        "\n",
        "$$\n",
        "f(x) = 4x^2 - 4x + 1.\n",
        "$$\n",
        "\n",
        "We can verify easily that this is a $\\mathcal{C}^2$ function -- its first and second derivatives exist, namely:\n",
        "\n",
        "$$\n",
        "f'(x) = 8x - 4 \\quad \\text{and} \\quad f''(x) = 8,\n",
        "$$\n",
        "\n",
        "and clearly the function $f''(x) = 8$ is continuous (it is a constant function). It is also $\\beta$-smooth. Its \"Hessian\" is simply a $1 \\times 1$ matrix with a single entry: $[8]$. This means that it is $8$-smooth ($\\beta$-smooth with $\\beta = 8$), so gradient descent with a step size of $\\eta = 1/8$ should make be sufficient to make our function values smaller."
      ],
      "metadata": {
        "id": "esnN2PnRKUcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    return 4 * x[0] ** 2 - 4 * x[0] + 1\n",
        "\n",
        "def grad_f(x):\n",
        "    return np.array([8 * x[0] - 4])"
      ],
      "metadata": {
        "id": "290es2LK0u_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.linspace(-5, 5, 100).reshape(100, 1)\n",
        "Y = f([X])\n",
        "plt.plot(X, Y)\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$f(x)$')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2tkl1cZQMMW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3 [3 points]\n",
        "Run `grad_desc` on the function above using $\\eta= 1/8$, $\\mathbf{x}_0 = -2$, a stopping condition of $0.01$, and `max_iter=500` (the default). saving the resulting `xs` and `fs`. Then, using `plot_grad_desc_1d` with the default values, plot the behavior of gradient descent by inputting `f`, `xs`, and `fs`."
      ],
      "metadata": {
        "id": "ObVWiaPqMC0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_grad_desc_1d(f, xs, fs, xmin=-3, xmax=3, spacing=200):\n",
        "  X = np.linspace(xmin, xmax, spacing).reshape(spacing, 1)\n",
        "  Y = f([X])\n",
        "  plt.plot(X, Y)\n",
        "  plt.plot(xs, fs, marker='o', linestyle='dashed', linewidth=2, markersize=4)\n",
        "  plt.xlabel('$x$')\n",
        "  plt.ylabel('$f(x)$')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "UcuFV9fdWgPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "eR-3-NlhN4Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see that the function value does, indeed, decrease as gradient descent moves along.\n",
        "\n",
        "As we talked about in class, the behavior of gradient descent becomes more erratic, however, as we increase the step size."
      ],
      "metadata": {
        "id": "Wma9HlQ_O1SQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4 [6 points]\n",
        "(i) Run `grad_desc` on the function above using $\\eta= 1/3$, $\\mathbf{x}_0 = -2$, a stopping condition of $0.01$, and `max_iter=500` (the default). saving the resulting `xs` and `fs`. Use `plot_grad_desc_1d` to plot the behavior of gradient descent using this step size.\n",
        "\n",
        "(ii) Run `grad_desc` on the function above using $\\eta = 1/2$, $\\mathbf{x}_0 = -2$, a stopping condition of $0.01$, and `max_iter=500` (the default). saving the resulting `xs` and `fs`. Use `plot_grad_desc_1d` to plot the behavior of gradient descent using this step size.\n",
        "\n",
        "(iii), (iv), (v) Now, experiment with three other step sizes $\\eta_3, \\eta_4, \\eta_5$ of your choosing (this is up to you), but with the same $\\mathbf{x}_0 = -2$, a stopping condition of $0.01$, and `max_iter=500`.\n",
        "\n",
        "(vi) Write a comment in the code cell titled `# YOUR COMMENT HERE (vi) #` that details your choices of $\\eta_3, \\eta_4,$ and $\\eta_5$ along with a verbal explanation of the behavior of gradient descent with each choice. This can just be a description of the visual behavior of the graphs with the different learning rates. How do they compare to the choices of $\\eta = 1/8$, $\\eta = 1/3$, and $\\eta = 1/2$?"
      ],
      "metadata": {
        "id": "uoOLuJnsPp6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE (i) #"
      ],
      "metadata": {
        "id": "OuvU6SePPtnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE (ii) #"
      ],
      "metadata": {
        "id": "L4xsIbtuQToC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE (iii) #"
      ],
      "metadata": {
        "id": "DMZvIjODQU_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE (iv) #"
      ],
      "metadata": {
        "id": "E-OBKwYcRWiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE (v) #"
      ],
      "metadata": {
        "id": "-85EvV0rRXcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR COMMENT HERE (vi) #"
      ],
      "metadata": {
        "id": "hl1IvtxRRYrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All we know about gradient descent so far, however, is that it gets us to a *local minimum*, which might not be a global minimum. We were lucky that $f(x) = 4x^2 - 4x + 1$ was \"bowl-shaped,\" (we will learn that the specific term for this is *convex*). However, in many cases, we will need to optimize non-convex functions. In those cases, unfortunately, gradient descent may get stuck at local minima."
      ],
      "metadata": {
        "id": "RifWEtzQRlhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ given by\n",
        "\n",
        "$$\n",
        "f(x) = \\sin(x) + \\cos(\\sqrt{2} x) + \\sin(\\sqrt{3} x).\n",
        "$$\n",
        "\n",
        "Let's now consider tweaking not the learning rate $\\eta$, but, rather, the initialization point $x_0$.\n",
        "\n"
      ],
      "metadata": {
        "id": "eB5_evOBR83t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    return np.sin(x[0]) + np.cos(np.sqrt(2) * x[0]) + np.sin(np.sqrt(3) * x[0])\n",
        "\n",
        "def grad_f(x):\n",
        "    return np.array([np.cos(x[0]) - np.sqrt(2) * np.sin(np.sqrt(2) * x[0]) + np.sqrt(3) * np.cos(np.sqrt(3) * x[0])])"
      ],
      "metadata": {
        "id": "hN4doOj5S_Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.linspace(-10, 10, 200).reshape(200, 1)\n",
        "Y = f([X])\n",
        "plt.plot(X, Y)\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$f(x)$')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RDXATi5xTQ5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 5 [3 points]\n",
        "In this exercise, run `grad_desc` with learning rate $\\eta = 0.1$, stopping condition $0.01$, and `max_iter=500`. For each of the following three conditions numbered (i), (ii), and (iii), choose three separate initialization points such that the final iterate of gradient descent $f(x_T)$ satisfies the condition. Then, plot the behavior of gradient descent using the supplied `plot_grad_desc_1d` function, using `xmin=-10` and `xmax=10` to get the correct scale. This should result in three separate plots, in the corresponding numbered `# YOUR CODE HERE #` cells below.\n",
        "\n",
        "(i) $f(x_T) \\leq -2$\n",
        "\n",
        "(ii) $-2 < f(x_T) < -1.5$\n",
        "\n",
        "(iii) $-1.5 < f(x_T) < -1$"
      ],
      "metadata": {
        "id": "LTmaqoxgTaQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE (i) #"
      ],
      "metadata": {
        "id": "tcBrQAIOVISa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE (ii) #"
      ],
      "metadata": {
        "id": "Z_1OuHMmVKwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE (iii) #"
      ],
      "metadata": {
        "id": "3pGe8eRFVL5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent on quadratic forms\n",
        "Now, we will consider gradient descent in multiple variables, particularly with functions $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$. We will focus on quadratic forms because they have Hessians that we can easily characterize and visually see the difference in.\n",
        "\n",
        "Consider the function $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ defined by\n",
        "\n",
        "$$\n",
        "f(x_1, x_2) = \\begin{bmatrix}\n",
        "x_1 & x_2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "4 & -1 \\\\\n",
        "-1 & 2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x_1 \\\\ x_2\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Clearly, this is a quadratic form, and, expanded out, it looks like:\n",
        "\n",
        "$$\n",
        "f(x_1, x_2) = 4x_1 - 2x_1 x_2 + 2 x_2^2\n",
        "$$"
      ],
      "metadata": {
        "id": "uwvs5yzp0yGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 6 [2 points]\n",
        "By hand, determine the Hessian $\\nabla^2 f(\\mathbf{x})$ of the above quadratic form. Then, using `np.linalg.eig`, determine the eigenvalues of this quadratic form. This should tell you $\\beta$ for which the function is $\\beta$-smooth. Using this $\\beta$, use **Theorem (Gradient descent makes the function value smaller)** from class (stated above) with the appropriate learning rate $\\eta$, initialization $\\mathbf{x}_0 = (-5, 10)$, stopping tolerance $0.01$, and `max_iter=500`. Save `xs` and `fs`.\n",
        "\n",
        "The provided functions `plot_grad_desc_2d` and `plot_grad_desc_2d_contour` should allow you to visualize the behavior of gradient descent. In the code cells below, use these functions function to plot the behavior of gradient descent."
      ],
      "metadata": {
        "id": "sl14m5YWY_Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_grad_desc_2d(f, xs, fs, xmin=-10, xmax=10, ymin=-10, ymax=10, spacing=200):\n",
        "    X, Y = np.meshgrid(np.linspace(xmin, xmax, spacing), np.linspace(ymin, ymax, spacing))\n",
        "    X = np.linspace(xmin, xmax, spacing)\n",
        "    Y = np.linspace(ymin, ymax, spacing)\n",
        "    X, Y = np.meshgrid(X, Y)\n",
        "    Z = f([X, Y])\n",
        "\n",
        "    # Plot in 3D\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.plot_surface(X, Y, Z, facecolors = cm.jet(Z/np.amax(Z)))\n",
        "    xs = np.array(xs)\n",
        "    ax.plot(xs[:, 0], xs[:, 1], fs, marker='o', linestyle='dashed', color='black', linewidth=2, markersize=4)\n",
        "    ax.set_xlabel('$x_1$')\n",
        "    ax.set_ylabel('$x_2$')\n",
        "    plt.show()\n",
        "\n",
        "def plot_grad_desc_2d_contour(f, xs, fs, xmin=-10, xmax=10, ymin=-10, ymax=10, spacing=100):\n",
        "    X = np.linspace(xmin, xmax, spacing)\n",
        "    Y = np.linspace(ymin, ymax, spacing)\n",
        "    X, Y = np.meshgrid(X, Y)\n",
        "    Z = f([X, Y])\n",
        "\n",
        "    # Plot contour\n",
        "    cmap = plt.colormaps[\"jet\"]\n",
        "    plt.contourf(X, Y, Z, spacing, cmap=cmap)\n",
        "    xs = np.array(xs)\n",
        "    plt.plot(xs[:, 0], xs[:, 1], marker='o', linestyle='dashed', color='black', linewidth=2, markersize=4)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$x_1$')\n",
        "    plt.ylabel('$x_2$')\n",
        "    plt.show()\n",
        "\n",
        "def f(x):\n",
        "    return 4 * x[0] - 2 * x[0] * x[1] + 2 * x[1] ** 2\n",
        "\n",
        "def grad_f(x):\n",
        "    return np.array([4 * x[0] - 2 * x[1], -2 * x[0] + 4 * x[1]])"
      ],
      "metadata": {
        "id": "dtiQtcqdall4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "HycJvVrBaz6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 7 [3 points]\n",
        "This exercise is a bit of a \"choose your own adventure.\" Following the template above for `f` and `grad_f`, construct three functions $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ following the specifications in (i), (ii), and (iii) and plot their gradient descent behavior using `grad_desc`, `plot_grad_desc_2d` and `plot_grad_desc_2d_contour`.\n",
        "\n",
        "(i) A positive definite quadratic form with $\\lambda_1, \\lambda_2 > 0$.\n",
        "\n",
        "(ii) A positive semidefinite quadratic form with one eigenvalue $\\lambda_1 > 0$ and $\\lambda_2 = 0$.\n",
        "\n",
        "(iii) An indefinite quadratic form with one eigenvalue $\\lambda_1 > 0$ and the other eigenvalue $\\lambda_2 < 0$.\n",
        "\n",
        "\n",
        "For the hyperparameters, use $\\eta$ set according to **Theorem (Gradient descent makes function value smaller)** stated above (you may determine the $\\beta$-smoothness through `np.linalg.eig`). Use a stopping tolerance of $0.01$ and `max_iter=500`. You may initialize $\\mathbf{x}_0$ anywhere you'd like, but try to choose one so that gradient descent runs for at least a fair amount of steps observable from the plots."
      ],
      "metadata": {
        "id": "cT1mad35dvA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE (i) #"
      ],
      "metadata": {
        "id": "DiTTi9bXYx9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE (ii) #"
      ],
      "metadata": {
        "id": "qlu3KYjffTyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE (iii) #"
      ],
      "metadata": {
        "id": "YclFt2hOfVMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}