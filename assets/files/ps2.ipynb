{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Math for ML PS 2 Programming Part\n",
        "[25 points] In this programming part, you will apply eigendecomposition to implement a common dimensionality reduction and data analysis technique from machine learning: [principal components analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis). A couple of these examples are modifications of exercises from Jake VanderPlas' excellent [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html).\n",
        "\n",
        "To submit, just fill in all the code cells with:\n",
        "\n",
        "```\n",
        "# YOUR CODE HERE #\n",
        "```\n",
        "\n",
        "You should turn in this `.ipynb` notebook into Gradescope per the homework sbumission instructions on the course webpage."
      ],
      "metadata": {
        "id": "wJ6yyfn9ts_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RKMp5attYpv"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eigendecomposition for Dimensionality Reduction\n",
        "First, we'll consider a simple example of a synthetic dataset in $\\mathbb{R}^2$. We'll first generate $n = 300$ random points, $\\mathbf{x}_1, \\dots, \\mathbf{x}_{300} \\in \\mathbb{R}^2$."
      ],
      "metadata": {
        "id": "6ZnuPMkTuapd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 300\n",
        "d = 2\n",
        "\n",
        "rng = np.random.RandomState(1)\n",
        "X = np.dot(rng.rand(d, d), rng.randn(d, n)).T\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iHJGtsTauYge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of X: {}\".format(X.shape))"
      ],
      "metadata": {
        "id": "e6tkjDgYdTb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although $\\mathbf{X}$ is not symmetric (it's not even square, for that matter), the matrix $\\mathbf{X}^{\\top} \\mathbf{X}$ is. The matrix $\\mathbf{X}^{\\top} \\mathbf{X}$ is called the *covariance matrix* of the data; in a rough sense, it explains the variance of the $d = 2$ features in terms of the $n = 300$ data points we have. We do not have notions of probability in this course yet, so we will leave it at that for now.\n",
        "\n",
        "However, we do have notions of linear algebra. We know that symmetric matrices have eigendecompositions, by the spectral theorem."
      ],
      "metadata": {
        "id": "FhUdajehdeQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1 [3 points]\n",
        "Write code below to find the eigendecomposition of the matrix $\\mathbf{X}^{\\top} \\mathbf{X}$ using the function `np.linalg.eig()`. Recall that appending `.T` to a `numpy` array transposes the array. Print the resulting eigenvalues and eigenvectors."
      ],
      "metadata": {
        "id": "364QOKPFv3Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eigvals, eigvecs = np.linalg.eig(X.T @ X)"
      ],
      "metadata": {
        "id": "PUm1nrtweNP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "Zz5oIgrJwY_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations -- you've just implemented the famous principal components analysis (PCA), which is just a synonym for \"eigendecomposition of the covariance matrix.\" To see that, let's use `sklearn`'s official PCA implementation to check."
      ],
      "metadata": {
        "id": "Kf9L9cdhe0Ut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2 [3 points]\n",
        "Use `sklearn` to run PCA; the [documentation is here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). To do this, set `n_components=2` and keep the rest of the hyperparameters as default. After fitting the `PCA` object, use `print(pca.components_)` and `print(pca.explained_variance_)` to print the two \"principal components\" of the data and the variance of the data they explain."
      ],
      "metadata": {
        "id": "_ndWyJx0yNqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "8N_DEKReyJ2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The components that PCA returns should be *about* the same as the eigenvectors you obtained in Exercise 1 (a discrepancy in numerical precision and the algorithm that `PCA` is running under the hood accounts for this), but the eigenvalues and `explained_variance_` entries shouldn't match up. This is because the only difference is that PCA performs eigendecomposition on a slightly different matrix: $\\frac{1}{n} \\mathbf{X}^{\\top} \\mathbf{X}$. This is known as the \"empirical covariance matrix,\" and it is still symmetric."
      ],
      "metadata": {
        "id": "rO__7KPb4eqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3 [3 points]\n",
        "Write code below to find the eigendecomposition of the matrix $\\frac{1}{n}\\mathbf{X}^{\\top} \\mathbf{X}$ using the function `np.linalg.eig()`. Recall that appending `.T` to a `numpy` array transposes the array. Print the resulting eigenvalues and eigenvectors. Do this yourself: compare to the `components_` and `explained_variance_` values obtained from PCA."
      ],
      "metadata": {
        "id": "b201iWW9g7wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "Wkmeb4eGgXnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_vector(v0, v1, ax=None):\n",
        "    ax = ax or plt.gca()\n",
        "    arrowprops=dict(arrowstyle='->',\n",
        "                    color='black',\n",
        "                    linewidth=2,\n",
        "                    shrinkA=0, shrinkB=0)\n",
        "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
        "\n",
        "def plot_components(X, eigvecs, eigvals):\n",
        "  # plot data\n",
        "  plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "  for length, vector in zip(eigvals, eigvecs):\n",
        "      v = vector * 3 * np.sqrt(length)\n",
        "      draw_vector(X.mean(axis=0), X.mean(axis=0) + v)\n",
        "  plt.axis('equal')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "CLCYDeg00fx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4 [3 points]\n",
        "Use the function `plot_components()` supplied above to plot the eigenvectors and eigenvalues of $\\frac{1}{n} \\mathbf{X}^{\\top} \\mathbf{X}$ atop the data."
      ],
      "metadata": {
        "id": "BDKIisY2ir6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "3am8w6ini4gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This should make clear that the famous technique of *principal components analysis (PCA)* is no more than doing eigendecomposition on the empirical covariance matrix of your collected data. The *principal components* correspond to the eigenvectors; the *explained variance* corresponds to the eigenvalues."
      ],
      "metadata": {
        "id": "l25U6GsCmrlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, recall that the projection operator for a 1D subspace spanned by the vector $\\mathbf{u} \\in \\mathbb{R}^d$ is given by the $d \\times d$ matrix\n",
        "$$\n",
        "P_{\\mathbf{u}} = \\frac{\\mathbf{u}\\mathbf{u}^{\\top}}{\\mathbf{u}^{\\top}\\mathbf{u}}.\n",
        "$$\n",
        "To calculate a projection for a single sample, $\\mathbf{x} \\in \\mathbb{R}^2$, we simply compute the matrix-vector product $P_{\\mathbf{u}} \\mathbf{x}$. To calculate the projection for a matrix of training examples $\\mathbf{X} \\in \\mathbb{R}^{300 \\times 2}$, we need to make sure that all the *columns* of $\\mathbf{X}$ include the examples, so we need to transpose $\\mathbf{X}$, do a matrix-matrix multiplication, and take the transpose again: $(P_{\\mathbf{u}} \\mathbf{X}^{\\top})^{\\top} \\in \\mathbb{R}^{300 \\times 2}$.\n",
        "\n",
        "We'll now use the eigenvectors of $\\frac{1}{n} \\mathbf{X}^{\\top}\\mathbf{X}$ to project the matrix of examples down onto its \"principal components\""
      ],
      "metadata": {
        "id": "qqPRLExfk6OH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 5 [3 points]\n",
        "Let the eigenvectors of $\\frac{1}{n} \\mathbf{X}^{\\top} \\mathbf{X}$ be denoted as $\\mathbf{u}_1, \\mathbf{u}_2$, with corresponding eigenvalues $\\lambda_1 \\geq \\lambda_2$. Compute the $2 \\times 2$ projection matrix $P_{\\mathbf{u}_1}$ for the 1D subspace spanned by $\\mathbf{u}_1$ and use the function `plot_project` below to plot the projection atop the original data."
      ],
      "metadata": {
        "id": "O_DKti_UnA9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_project(X, P_u):\n",
        "  X_proj = (P_u @ X.T).T\n",
        "  plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "  plt.scatter(X_proj[:, 0], X_proj[:, 1], alpha=0.8)\n",
        "  plt.axis('equal')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "9JnpuLDahagW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "giLyU8IJhMSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that we've reduced the data to the single dimensional subspace spanned by $\\mathbf{u}_1$."
      ],
      "metadata": {
        "id": "0xoessyioS42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at another example in $\\mathbb{R}^2$ just to drive the point home. We'll again generate $n = 300$ random examples in $d = 2$ dimensions. However, this time, we'll generate the data to have more equal spread in both directions."
      ],
      "metadata": {
        "id": "Ivn2xZVIp18k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 300\n",
        "d = 2\n",
        "\n",
        "mu = [0, 0]\n",
        "cov = np.array([[2, 0],\n",
        "                [0, 2]])\n",
        "X = np.random.multivariate_normal(mu, cov, size=(300,))\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6L7KmBSRoxR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of X: {}\".format(X.shape))"
      ],
      "metadata": {
        "id": "rmWH9QecqVik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 6 [3 points]\n",
        "Write code below to find the eigendecomposition of the matrix $\\frac{1}{n}\\mathbf{X}^{\\top} \\mathbf{X}$ using the function `np.linalg.eig()`. Recall that appending `.T` to a `numpy` array transposes the array. Print the resulting eigenvalues and eigenvectors.\n",
        "\n",
        "Use the function `plot_components()` supplied above to plot the eigenvectors and eigenvalues of $\\frac{1}{n} \\mathbf{X}^{\\top} \\mathbf{X}$ atop the data."
      ],
      "metadata": {
        "id": "FiuOyfnOqU_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "MxoDX7djqUYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA for Eigenfaces\n",
        "One domain in which we can use eigendcomposition/PCA is facial recognition. In this series of exercises, we'll use eigendecomposition to find the eigenvectors of a much more high dimensional dataset of face images. These eigenvectors will turn out to be \"component faces\" that we can linearly combine to construct the face images in the dataset.\n",
        "\n",
        "We'll first load the [Labeled Faces in the Wild dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html) from `scikit-learn`. This might take a minute or so to load, depending on your internet connection."
      ],
      "metadata": {
        "id": "Yz0MqiYArYpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "faces = fetch_lfw_people(min_faces_per_person=60)\n",
        "print(faces.target_names)\n",
        "print(faces.images.shape)"
      ],
      "metadata": {
        "id": "vNP1mxzFifoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the first four faces in the dataset."
      ],
      "metadata": {
        "id": "rZY_rXXItwTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 4, figsize=(10, 8))\n",
        "for i, ax in enumerate(axes):\n",
        "  ax.set_title(faces.target_names[faces.target[i]])\n",
        "  ax.imshow(faces.images[i], cmap='gray')\n",
        "  ax.grid(False)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RvduJzCQs84B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the above code generates $n = 1,348$ face images, but each face image is a matrix with dimensions $62 \\times 47$. In order to deal with each of the faces as vectors, we'll \"flatten\" the face matrices into vectors so the real dataset we deal with is a matrix $\\mathbf{X} \\in \\mathbb{R}^{1348 \\times 2914}$. Here, $n = 1348$ and $d = 2914$, and we are in a very high-dimensional setting."
      ],
      "metadata": {
        "id": "pfaPjwj7sRWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = faces.data\n",
        "print(\"Shape of X: {}\".format(X.shape))"
      ],
      "metadata": {
        "id": "iBq8rML1pbvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 7 [4 points]\n",
        "Use `PCA` from `sklearn.decomposition` to compute the first $150$ eigenvectors (in `pca.components_`) and eigenvalues (in `pca.explained_variance_`). We wil use this instead of `np.linalg.eig` because it allows us to only compute the eigenvalues/eigenvectors up until a certain cutoff, which saves time in high-dimensional situations such as this one. In order to do this, create a `PCA()` object with `n_components=150`. Print the first $5$ eigenvectors and eigenvalues. Then, use `show_eigenfaces` with the eigenvectors you found to show the first $24$ eigenvectors and use `plot_eigvals` to plot the eigenvalues.\n",
        "\n",
        "The [official documentation for `PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) may help."
      ],
      "metadata": {
        "id": "NLYuDGYkt5Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_eigenfaces(eigvecs):\n",
        "  '''\n",
        "  NOTE: expects eigvecs to be a matrix of shape (n, d), where n is the\n",
        "  number of eigenvectors and d is the dimension of each eigenvector. That is,\n",
        "  eigvecs should be a matrix with eigenvectors arranged row-wise.\n",
        "  '''\n",
        "  fig, axes = plt.subplots(3, 8, figsize=(9, 4),\n",
        "                          subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                          gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "  for i, ax in enumerate(axes.flat):\n",
        "      ax.imshow(eigvecs[i].reshape(62, 47), cmap='gray')\n",
        "  plt.show()\n",
        "\n",
        "def plot_eigvals(eigvals):\n",
        "  '''\n",
        "  NOTE: expects eigvals to be a list of eigenvalues.\n",
        "  '''\n",
        "  plt.plot(eigvals, 'bo')\n",
        "  plt.xlabel('index of $\\lambda_i$')\n",
        "  plt.ylabel('value of $\\lambda_i$')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Ak7sY8lryTdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "Q-lrZckHpq0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the eigenvalue distribution in `plot_eigvals` that much of the \"information\" of the dataset is packed into the first $150$ eigenvectors. Recall that, for an orthonormal basis $\\mathbf{u}_1, \\dots, \\mathbf{u}_k \\in \\mathbb{R}^d$ spanning a $k$-dimensional subspace $\\mathcal{X}$, the projection onto that subspace is obtained by organizing the semi-orthogonal matrix $\\mathbf{U} \\in \\mathbb{R}^{d \\times k}$ with $\\mathbf{u}_1, \\dots, \\mathbf{u}_k$ as the columns and constructing the $d \\times d$ projection matrix:\n",
        "\n",
        "$$\n",
        "P_{\\mathcal{X}} = \\mathbf{U} \\mathbf{U}^{\\top}.\n",
        "$$\n",
        "\n",
        "Now, we will project the first $10$ faces onto the $k = 150$ dimension subspace. This reduces the dimensionality of the problem from $d = 2914$ to $k = 150$, which is a huge savings. We also see that this doesn't reduce the image quality too much."
      ],
      "metadata": {
        "id": "oX2aKwe0y7VR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 8 [3 points]\n",
        "Using the $150$ eigenvectors you found from Exercise 7, construct the projection matrix onto the $150$-dimensional subspace spanned by the eigenvectors, $P_{\\mathcal{X}} = \\mathbf{U} \\mathbf{U}^{\\top}$. Apply the projection matrix to each example in $\\mathbf{X}$ (you will need to do some appropriate transposing), and use `plot_projected` to visualize the discrepancy between the original faces and the projected faces."
      ],
      "metadata": {
        "id": "-I0lmkvd0y6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_projected(X, X_proj):\n",
        "  '''\n",
        "  NOTE: The argument X expects a numpy array of shape (n, d) and the argument\n",
        "  X_proj expects a numpy array of shape (n, k), where k < d is the dimension of\n",
        "  the projected subspace.\n",
        "  '''\n",
        "  # Plot the results\n",
        "  k = X_proj.shape[1]\n",
        "  fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),\n",
        "                        subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                        gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "  for i in range(10):\n",
        "      ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n",
        "      ax[1, i].imshow(X_proj[i].reshape(62, 47), cmap='binary_r')\n",
        "\n",
        "  ax[0, 0].set_ylabel('full-dim\\ninput')\n",
        "  ax[1, 0].set_ylabel('{}-dim\\nprojection'.format(k))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "i5c2uM7HzCuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "b9fj8cjmzj-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that projecting these faces onto the space spanned by the first $150$ eigenvectors still leads to recognizable, albeit blurrier, results."
      ],
      "metadata": {
        "id": "CeJ1cFDu2MKw"
      }
    }
  ]
}