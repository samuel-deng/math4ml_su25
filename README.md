---
layout: home
title: Home
nav_exclude: false
permalink: /:path/
seo:
  type: Course
  name: Mathematics for Machine Learning
nav_order: 1
---

# Welcome to Mathematics for Machine Learning!
Welcome to the main course webpage for *Mathematics for Machine Learning*! This is a course taught at Columbia University during the
**Summer B semester of 2024** by [Samuel Deng](https://samuel-deng.github.io/). If you're a student, all course announcements, logistics, and materials will be available on
this page for ease of access (check here first instead of Courseworks). If you're just dropping by, I hope this course is useful to you.

**What's this course?** This is a topics course meant to strengthen the mathematical fundamentals for students wishing to
pursue further study in machine learning. The serious study of machine learning requires a student
to be proficient in several prerequisite subjects: (i) linear algebra, (ii) multivariable calculus, and
(iii) probability and statistics. This course assumes that the student has already taken courses
in these subjects at the undergraduate level (it is not a replacement), but would like to be more
comfortable with their mathematical maturity in any of these areas before approaching a formal
course in machine learning at the level of, say, COMS W4771 (Machine Learning) at Columbia. We will not give 
comprehensive treatment of each of these areas; instead, we will present the main results that are most relevant to 
the analysis and design of machine learning models.

This is a course with a loose story. The course is structured around two main ideas that underlie modern machine learning: 
*least squares regression* and *gradient descent.* Very informally, least squares regression is a classic way of modeling
problems in machine learning (the *"what"*), and gradient descent is the workhorse algorithm that drives much of modern
machine learning (the *"how"*). Every week, we'll develop and motivate these two ideas in lecture with the tools and
concepts you learn from each part of the course. As the class goes on, you'll develop different perspectives on these two
ideas from, first, what we learn in linear algebra, then calculus and optimization, and, finally, probability and
statistics. The hope is that, by the end of the course, you'll have a deep understanding of both these ideas in ML while
also having two concrete "applications" to motivate all the abstract mathematical tools and concepts you learn in the course.

See [Syllabus]({{ site.baseurl }}{% link syllabus.md %}) for the full syllabus.
 
**Contact.** If you have any questions, feedback, or just want to chat about this course, email me at [samdeng@cs.columbia.edu](mailto:samdeng@cs.columbia.edu).

**Feedback?** By the nature of this course, students will come from widely different levels of background, and it
is my job to make sure that no student is left behind or glossed over because of this. To this end,
if thereâ€™s anything I can do to help you learn better, do not hesitate to contact me directly or [leave anonymous
feedback at this link](https://forms.gle/69VUPBKcma283wre7).

**Course philosophy.** The goal of this course is to reinforce and deepen important mathematical fundamentals, gain
better intuition of these mathematical tools, and develop confidence in mathematical maturity. All
of these require work that may sometimes seem daunting, but I believe that any student is capable
of growing in the course, so long as they continually grapple with the concepts and do the work.
This may, at times, be difficult, but struggle is a totally normal part of the process. I was in your
shoes, at one point (and still am!), and I can assure you that many of these concepts seem really
difficult until they inevitably, after plugging away for a while, become natural. I hope you, the
student, come away with this feeling as well.